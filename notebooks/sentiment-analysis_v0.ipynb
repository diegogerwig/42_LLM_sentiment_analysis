{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Transfer Learning and Fine-tuning\n",
    "\n",
    "This notebook demonstrates how to fine-tune a pre-trained model for a binary sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Using cached triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Using cached tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-18.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Using cached torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Using cached scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "Using cached datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached pyarrow-18.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Using cached scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, xxhash, threadpoolctl, sympy, scipy, safetensors, requests, regex, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, joblib, fsspec, filelock, dill, triton, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Not uninstalling requests at /opt/conda/lib/python3.11/site-packages, outside environment /opt/.qbraid/environments/qbraid_000000/pyenv\n",
      "    Can't uninstall 'requests'. No files were found to uninstall.\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Not uninstalling fsspec at /opt/conda/lib/python3.11/site-packages, outside environment /opt/.qbraid/environments/qbraid_000000/pyenv\n",
      "    Can't uninstall 'fsspec'. No files were found to uninstall.\n",
      "Successfully installed datasets-3.1.0 dill-0.3.8 filelock-3.16.1 fsspec-2024.9.0 huggingface-hub-0.26.2 joblib-1.4.2 mpmath-1.3.0 multiprocess-0.70.16 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pyarrow-18.0.0 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.20.3 torch-2.5.1 transformers-4.46.3 triton-3.1.0 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.11/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (8.24.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.11.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting ydata-profiling\n",
      "  Using cached ydata_profiling-4.12.0-py2.py3-none-any.whl.metadata (20 kB)\n",
      "Collecting scipy<1.14,>=1.4.1 (from ydata-profiling)\n",
      "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: pandas!=1.4.0,<3,>1.1 in /opt/conda/lib/python3.11/site-packages (from ydata-profiling) (2.2.3)\n",
      "Requirement already satisfied: matplotlib<3.10,>=3.5 in /opt/conda/lib/python3.11/site-packages (from ydata-profiling) (3.9.2)\n",
      "Requirement already satisfied: pydantic>=2 in /opt/conda/lib/python3.11/site-packages (from ydata-profiling) (2.9.2)\n",
      "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from ydata-profiling) (6.0.1)\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /opt/conda/lib/python3.11/site-packages (from ydata-profiling) (3.1.4)\n",
      "Collecting visions<0.7.7,>=0.7.5 (from visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling)\n",
      "  Using cached visions-0.7.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.2,>=1.16.0 in /opt/conda/lib/python3.11/site-packages (from ydata-profiling) (1.26.4)\n",
      "Collecting htmlmin==0.1.12 (from ydata-profiling)\n",
      "  Using cached htmlmin-0.1.12-py3-none-any.whl\n",
      "Collecting phik<0.13,>=0.11.1 (from ydata-profiling)\n",
      "  Using cached phik-0.12.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2.24.0 in /opt/.qbraid/environments/qbraid_000000/pyenv/lib/python3.11/site-packages (from ydata-profiling) (2.32.3)\n",
      "Requirement already satisfied: tqdm<5,>=4.48.2 in /opt/conda/lib/python3.11/site-packages (from ydata-profiling) (4.66.4)\n",
      "Collecting seaborn<0.14,>=0.10.1 (from ydata-profiling)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting multimethod<2,>=1.4 (from ydata-profiling)\n",
      "  Using cached multimethod-1.12-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting statsmodels<1,>=0.13.2 (from ydata-profiling)\n",
      "  Using cached statsmodels-0.14.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting typeguard<5,>=3 (from ydata-profiling)\n",
      "  Using cached typeguard-4.4.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting imagehash==4.3.1 (from ydata-profiling)\n",
      "  Using cached ImageHash-4.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting wordcloud>=1.9.3 (from ydata-profiling)\n",
      "  Using cached wordcloud-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting dacite>=1.8 (from ydata-profiling)\n",
      "  Using cached dacite-1.8.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting numba<1,>=0.56.0 (from ydata-profiling)\n",
      "  Using cached numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting PyWavelets (from imagehash==4.3.1->ydata-profiling)\n",
      "  Using cached pywavelets-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (from imagehash==4.3.1->ydata-profiling) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2<3.2,>=2.11.1->ydata-profiling) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (2.9.0)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba<1,>=0.56.0->ydata-profiling)\n",
      "  Using cached llvmlite-0.43.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas!=1.4.0,<3,>1.1->ydata-profiling) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas!=1.4.0,<3,>1.1->ydata-profiling) (2024.2)\n",
      "Requirement already satisfied: joblib>=0.14.1 in /opt/.qbraid/environments/qbraid_000000/pyenv/lib/python3.11/site-packages (from phik<0.13,>=0.11.1->ydata-profiling) (1.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2->ydata-profiling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2->ydata-profiling) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2->ydata-profiling) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.24.0->ydata-profiling) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.24.0->ydata-profiling) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.24.0->ydata-profiling) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.24.0->ydata-profiling) (2024.2.2)\n",
      "Collecting patsy>=0.5.6 (from statsmodels<1,>=0.13.2->ydata-profiling)\n",
      "  Using cached patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /opt/conda/lib/python3.11/site-packages (from visions<0.7.7,>=0.7.5->visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling) (23.2.0)\n",
      "Requirement already satisfied: networkx>=2.4 in /opt/.qbraid/environments/qbraid_000000/pyenv/lib/python3.11/site-packages (from visions<0.7.7,>=0.7.5->visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling) (3.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib<3.10,>=3.5->ydata-profiling) (1.16.0)\n",
      "Using cached ydata_profiling-4.12.0-py2.py3-none-any.whl (390 kB)\n",
      "Using cached ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "Using cached dacite-1.8.1-py3-none-any.whl (14 kB)\n",
      "Using cached multimethod-1.12-py3-none-any.whl (10 kB)\n",
      "Using cached numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "Using cached phik-0.12.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (687 kB)\n",
      "Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached statsmodels-0.14.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "Using cached typeguard-4.4.1-py3-none-any.whl (35 kB)\n",
      "Using cached visions-0.7.6-py3-none-any.whl (104 kB)\n",
      "Using cached wordcloud-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (547 kB)\n",
      "Using cached llvmlite-0.43.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "Using cached patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Using cached pywavelets-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "Installing collected packages: htmlmin, typeguard, scipy, PyWavelets, patsy, multimethod, llvmlite, dacite, numba, imagehash, wordcloud, visions, statsmodels, seaborn, phik, ydata-profiling\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed PyWavelets-1.7.0 dacite-1.8.1 htmlmin-0.1.12 imagehash-4.3.1 llvmlite-0.43.0 multimethod-1.12 numba-0.60.0 patsy-1.0.1 phik-0.12.4 scipy-1.13.1 seaborn-0.13.2 statsmodels-0.14.4 typeguard-4.4.1 visions-0.7.6 wordcloud-1.9.4 ydata-profiling-4.12.0\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers pandas scikit-learn datasets\n",
    "%pip install --upgrade ipywidgets\n",
    "%pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, DistilBertModel, DistilBertForSequenceClassification,\n",
    "    BertTokenizer, BertModel,\n",
    "    RobertaTokenizer, RobertaModel\n",
    ")\n",
    "import time\n",
    "import psutil\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== System Info ===\n",
      "CPU cores: 8\n",
      "RAM: 31.35 GB\n",
      "\n",
      "=== GPU Info ===\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=== System Info ===\")\n",
    "print(f\"CPU cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024 ** 3):.2f} GB\")\n",
    "print(\"\\n=== GPU Info ===\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label  idx\n",
      "0  at least one scene is so disgusting that viewe...      0  413\n",
      "1  even the finest chef ca n't make a hotdog into...      0  701\n",
      "2  collateral damage finally delivers the goods f...      1  834\n",
      "3  exciting and direct , with ghost imagery that ...      1  821\n",
      "4  and when you 're talking about a slapstick com...      0  748\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  150 non-null    object\n",
      " 1   label     150 non-null    int64 \n",
      " 2   idx       150 non-null    int32 \n",
      "dtypes: int32(1), int64(1), object(1)\n",
      "memory usage: 3.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_sample = pd.read_parquet('../data/sample_reviews.parquet')\n",
    "\n",
    "print(df_sample.head())\n",
    "print(df_sample.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the datasets reviews from Hugging Face:\n",
    "\n",
    "https://huggingface.co/datasets/stanfordnlp/imdb\n",
    "\n",
    "https://huggingface.co/datasets/SetFit/amazon_reviews_multi_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"stanfordnlp/imdb\"\n",
    "# data_name = \"SetFit/amazon_reviews_multi_en\"\n",
    "\n",
    "dataset = load_dataset(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced train data saved in ./data/imdb_train_balanced.csv: 300 samples\n",
      "Balanced test data saved in ./data/imdb_test_balanced.csv: 300 samples\n",
      "\n",
      "Class distribution in Train:\n",
      "label\n",
      "0    150\n",
      "1    150\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in Test:\n",
      "label\n",
      "0    150\n",
      "1    150\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SAMPLE_SIZE = 300\n",
    "\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "train_samples_per_class = SAMPLE_SIZE // len(train_df['label'].unique())\n",
    "test_samples_per_class = SAMPLE_SIZE // len(test_df['label'].unique())\n",
    "\n",
    "train_balanced = pd.concat([\n",
    "    train_df[train_df['label'] == label].sample(n=train_samples_per_class, random_state=42)\n",
    "    for label in train_df['label'].unique()\n",
    "])\n",
    "\n",
    "test_balanced = pd.concat([\n",
    "    test_df[test_df['label'] == label].sample(n=test_samples_per_class, random_state=42)\n",
    "    for label in test_df['label'].unique()\n",
    "])\n",
    "\n",
    "name = data_name.split('/')[1]\n",
    "\n",
    "train_balanced.to_csv(f'../data/{name}_train_balanced.csv', index=False, encoding='utf-8')\n",
    "test_balanced.to_csv(f'../data/{name}_test_balanced.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Balanced train data saved in ./data/{name}_train_balanced.csv: {len(train_balanced)} samples\")\n",
    "print(f\"Balanced test data saved in ./data/{name}_test_balanced.csv: {len(test_balanced)} samples\")\n",
    "\n",
    "print(\"\\nClass distribution in Train:\")\n",
    "print(train_balanced['label'].value_counts())\n",
    "print(\"\\nClass distribution in Test:\")\n",
    "print(test_balanced['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>Wow, what a total let down! The fact people th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11919</th>\n",
       "      <td>If Bob Ludlum was to see this mini series, he ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8909</th>\n",
       "      <td>To call a film about a crippled ghost taking r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4963</th>\n",
       "      <td>they have sex with melons in Asia.&lt;br /&gt;&lt;br /&gt;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10099</th>\n",
       "      <td>Although the production and Jerry Jameson's di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "1766   Wow, what a total let down! The fact people th...      0\n",
       "11919  If Bob Ludlum was to see this mini series, he ...      0\n",
       "8909   To call a film about a crippled ghost taking r...      0\n",
       "4963   they have sex with melons in Asia.<br /><br />...      0\n",
       "10099  Although the production and Jerry Jameson's di...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_balanced.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 300 entries, 1766 to 19219\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    300 non-null    object\n",
      " 1   label   300 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 7.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train_balanced.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    150\n",
       "1    150\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_balanced['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length statistics per class:\n",
      "      text_length                                  \n",
      "       min_length max_length mean_length std_length\n",
      "label                                              \n",
      "0             210       4737     1291.05     866.78\n",
      "1             157       7068     1404.53    1171.32\n",
      "\n",
      "Overall text length statistics:\n",
      "         min_length  max_length  mean_length  std_length\n",
      "Overall         157        7068      1347.79     1030.21\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOverall text length statistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(overall_stats)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     25\u001b[0m train_balanced\u001b[38;5;241m.\u001b[39mboxplot(column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_length\u001b[39m\u001b[38;5;124m'\u001b[39m, by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText Length Distribution by Class\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "train_balanced['text_length'] = train_balanced['text'].str.len()\n",
    "\n",
    "length_stats = train_balanced.groupby('label').agg({\n",
    "    'text_length': [\n",
    "        ('min_length', 'min'),\n",
    "        ('max_length', 'max'),\n",
    "        ('mean_length', 'mean'),\n",
    "        ('std_length', 'std')\n",
    "    ]\n",
    "}).round(2)\n",
    "\n",
    "overall_stats = pd.DataFrame({\n",
    "    'min_length': [train_balanced['text_length'].min()],\n",
    "    'max_length': [train_balanced['text_length'].max()],\n",
    "    'mean_length': [train_balanced['text_length'].mean()],\n",
    "    'std_length': [train_balanced['text_length'].std()]\n",
    "}, index=['Overall']).round(2)\n",
    "\n",
    "print(\"Text length statistics per class:\")\n",
    "print(length_stats)\n",
    "print(\"\\nOverall text length statistics:\")\n",
    "print(overall_stats)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "train_balanced.boxplot(column='text_length', by='label')\n",
    "plt.title('Text Length Distribution by Class')\n",
    "plt.ylabel('Text Length')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "import webbrowser\n",
    "\n",
    "profile = ProfileReport(train_balanced, title=\"Dataset IMDB\")\n",
    "profile.to_notebook_iframe()  \n",
    "\n",
    "report_path = f\"../reports/ydata_report_{name}.html\"\n",
    "profile.to_file(report_path)\n",
    "webbrowser.open('file://' + os.path.realpath(report_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model and Platform Research and Selection\n",
    "\n",
    "Models:\n",
    "- BERT\n",
    "- RoBERTa\n",
    "- DistilBERT\n",
    "- GPT-2\n",
    "- Electra\n",
    "- XLNet\n",
    "\n",
    "**DistilBERT** is good balance between performance and computational efficiency. It is a lighter and faster version of BERT.\n",
    "\n",
    "Computing platforms:\n",
    "- AWS Sagemaker Studio Labs\n",
    "- QBraid\n",
    "- Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComparator:\n",
    "    def __init__(self):\n",
    "        self.models: Dict = {}\n",
    "        self.tokenizers: Dict = {}\n",
    "        self.results: List = []\n",
    "        \n",
    "    def load_model(self, model_name: str):\n",
    "        \"\"\"Loads a model and its tokenizer.\"\"\"\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        \n",
    "        model_configs = {\n",
    "            'distilbert': ('distilbert-base-uncased', DistilBertTokenizer, DistilBertModel),\n",
    "            'bert': ('bert-base-uncased', BertTokenizer, BertModel),\n",
    "            'roberta': ('roberta-base', RobertaTokenizer, RobertaModel),\n",
    "        }\n",
    "        \n",
    "        if model_name in model_configs:\n",
    "            model_path, TokenizerClass, ModelClass = model_configs[model_name]\n",
    "            try:\n",
    "                tokenizer = TokenizerClass.from_pretrained(model_path)\n",
    "                model = ModelClass.from_pretrained(model_path)\n",
    "                \n",
    "                # Special handling for GPT2 tokenizer\n",
    "                if model_name == 'gpt2':\n",
    "                    tokenizer.pad_token = tokenizer.eos_token\n",
    "                    \n",
    "                self.models[model_name] = model\n",
    "                self.tokenizers[model_name] = tokenizer\n",
    "                print(f\"Successfully loaded {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {model_name}: {str(e)}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    def measure_performance(self, model_name: str, text: str, num_runs: int = 5):\n",
    "        \"\"\"Measures model performance for a given text.\"\"\"\n",
    "        model = self.models[model_name]\n",
    "        tokenizer = self.tokenizers[model_name]\n",
    "        \n",
    "        # Performance measurements\n",
    "        total_time = 0\n",
    "        memory_usage = []\n",
    "        \n",
    "        # Prepare input\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # Special handling for XLNet\n",
    "        if model_name == 'xlnet':\n",
    "            inputs['token_type_ids'] = torch.zeros_like(inputs['input_ids'])\n",
    "        \n",
    "        # Multiple runs for averaging\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            total_time += (end_time - start_time)\n",
    "            memory_usage.append(psutil.Process().memory_info().rss / 1024 / 1024)  # MB\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_time = total_time / num_runs\n",
    "        avg_memory = np.mean(memory_usage)\n",
    "        \n",
    "        # Save results\n",
    "        self.results.append({\n",
    "            'model': model_name,\n",
    "            'avg_time_ms': round(avg_time * 1000, 2),\n",
    "            'avg_memory_mb': round(avg_memory, 2),\n",
    "            'parameters': sum(p.numel() for p in model.parameters()),\n",
    "            'input_length': len(inputs['input_ids'][0])\n",
    "        })\n",
    "    \n",
    "    def show_results(self):\n",
    "        \"\"\"Shows results in a DataFrame.\"\"\"\n",
    "        df = pd.DataFrame(self.results)\n",
    "        # Add relative speed comparison (normalized to BERT)\n",
    "        if 'bert' in df['model'].values:\n",
    "            bert_time = df[df['model'] == 'bert']['avg_time_ms'].values[0]\n",
    "            df['relative_speed'] = bert_time / df['avg_time_ms']\n",
    "        return df\n",
    "\n",
    "def main():\n",
    "    # Sample texts for analysis\n",
    "    texts = [\n",
    "        \"\"\"This is a longer text that allows us to see how different models behave\n",
    "        with more extensive content. We want to analyze the differences in processing\n",
    "        time and memory usage across various transformer architectures.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize comparator\n",
    "    comparator = ModelComparator()\n",
    "    \n",
    "    # Load all models\n",
    "    models = ['distilbert', 'bert', 'roberta']\n",
    "    for model in models:\n",
    "        try:\n",
    "            comparator.load_model(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {model} due to error: {str(e)}\")\n",
    "    \n",
    "    # Run tests\n",
    "    print(\"\\nRunning performance tests...\")\n",
    "    for text in texts:\n",
    "        print(f\"\\nTesting with text of {len(text)} characters\")\n",
    "        for model in comparator.models.keys():\n",
    "            print(f\"Testing {model}...\")\n",
    "            comparator.measure_performance(model, text)\n",
    "    \n",
    "    # Show results\n",
    "    results = comparator.show_results()\n",
    "    print(\"\\nComparison Results:\")\n",
    "    print(results.to_string(index=False))\n",
    "      \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model and Tokenizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\tdef __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "\t\tself.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length)\n",
    "\t\tself.labels = labels\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\titem = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\t\titem['labels'] = torch.tensor(self.labels[idx])\n",
    "\t\treturn item\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.labels)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs=3):\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tmodel.train()\n",
    "\t\tfor batch in train_loader:\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tinput_ids = batch['input_ids'].to(device)\n",
    "\t\t\tattention_mask = batch['attention_mask'].to(device)\n",
    "\t\t\tlabels = batch['labels'].to(device)\n",
    "\t\t\toutputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\n",
    "\t\tmodel.eval()\n",
    "\t\tval_accuracy = []\n",
    "\t\tfor batch in val_loader:\n",
    "\t\t\tinput_ids = batch['input_ids'].to(device)\n",
    "\t\t\tattention_mask = batch['attention_mask'].to(device)\n",
    "\t\t\tlabels = batch['labels'].to(device)\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\toutputs = model(input_ids, attention_mask=attention_mask)\n",
    "\t\t\tpredictions = torch.argmax(outputs.logits, dim=1)\n",
    "\t\t\taccuracy = (predictions == labels).float().mean().item()\n",
    "\t\t\tval_accuracy.append(accuracy)\n",
    "\t\t\n",
    "\t\tprint(f\"Epoch {epoch + 1}, Val Accuracy: {np.mean(val_accuracy)}\")\n",
    "\treturn model\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize text data\n",
    "train_texts = train_balanced['text'].tolist()\n",
    "train_labels = train_balanced['label'].tolist()\n",
    "test_texts = test_balanced['text'].tolist()\n",
    "test_labels = test_balanced['label'].tolist()\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "# Create PyTorch data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(inputs['labels'].cpu().tolist())\n",
    "    \n",
    "    accuracy = accuracy_score(actual_labels, predictions)\n",
    "    f1 = f1_score(actual_labels, predictions, average='binary')\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3  # Adjust based on your needs and time constraints\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    avg_train_loss = train_epoch(model, train_dataloader, optimizer, device)\n",
    "    print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    accuracy, f1 = evaluate(model, val_dataloader, device)\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}\")\n",
    "    print(\"----\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./models/sentiment_model')\n",
    "tokenizer.save_pretrained('./models/sentiment_model')\n",
    "print(\"Model saved in './modles/sentiment_model' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1)\n",
    "    \n",
    "    return \"Positive\" if prediction.item() == 1 else \"Negative\"\n",
    "\n",
    "# Test the model with some example reviews\n",
    "test_reviews = [\n",
    "    \"This movie was fantastic! I loved every minute of it.\",\n",
    "    \"Absolutely terrible. Waste of time and money.\",\n",
    "    \"It was okay, nothing special but not bad either.\"\n",
    "]\n",
    "\n",
    "for review in test_reviews:\n",
    "    sentiment = predict_sentiment(review, model, tokenizer, device)\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Predicted sentiment: {sentiment}\")\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [Default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
